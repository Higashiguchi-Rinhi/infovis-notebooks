{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "title: \"Final presentation 10 Jan\"\n",
        "author: \"Higashiguchi Rinhi\"\n",
        "format:\n",
        "  html:toc\n",
        "execute:\n",
        "  echo: false\n",
        "jupyter: python3\n",
        "---\n",
        "\n",
        "\n",
        "# 答えたい問い\n",
        "\n",
        "・世の中のメロディー大体そんなに遠い音に行かないのではないのか、音が移るのには規則があるのではないか。\\\n",
        "・音自体がどこに行くのが多いのか、1,2,5音とか。\\\n",
        "・（時間があれば）楽器によってそれが変わるのか\n",
        "\n",
        "# データの概要、取得と前処理の方法\n",
        "\n",
        "以下の論文からのデータセットを使用する。 ::: {.callout-note appearance=\"minimal\"} John Thickstun, Zaid Harchaoui, & Sham M. Kakade. (2016). MusicNet (1.0) \\[Data set\\]. Zenodo. https://doi.org/10.5281/zenodo.5120004 :::\n",
        "\n",
        "kaggle.comにあったデータの概要は以下のようになっている。 ::: {.callout-note appearance=\"minimal\"} MusicNet is a collection of 330 freely-licensed classical music recordings, together with over 1 million annotated labels indicating the precise time of each note in every recording, the instrument that plays each note, and the note's position in the metrical structure of the composition. The labels are acquired from musical scores aligned to recordings by dynamic time warping. The labels are verified by trained musicians; a labeling error rate of 4% has been estimated. The MusicNet labels are offered to the machine learning and music communities as a resource for training models and a common benchmark for comparing results.\n",
        "\n",
        "https://www.kaggle.com/datasets/imsparsh/musicnet-dataset ::: 実際のデータを見てみると\n",
        "\n",
        "1.  作曲家\n",
        "\n",
        "2.  曲名\n",
        "\n",
        "3.  楽章番号\n",
        "\n",
        "4.  アンサンブルの種類(solo, quartet etc.)\n",
        "\n",
        "5.  楽器\n",
        "\n",
        "6.  音符\n",
        "\n",
        "7.  音価\n",
        "\n",
        "8.  音の開始時間\n",
        "\n",
        "9.  音の終了時間\n",
        "\n",
        "    といった変数があり、今回気になるのは楽曲、楽章番号、音符、音の開始時間です。\n",
        "\n",
        "## データの前処理\n",
        "\n",
        "メタデータと曲ごとのデータがあったので、idを基に両者を合体した。基本的にtidy dataの形になっていた。データの欠測はないことを確認した。\n",
        "\n",
        "メタデータ上に気になる変数である楽器と音価、が見あたらなかったので、数値として入力されている解読をする。\n",
        "\n",
        "楽器: [ここから](https://music.stackexchange.com/questions/135779/im-having-difficulty-comprehending-the-timing-information-presented-in-the-csv)取得。\n",
        "\n",
        "-   1 is Piano (4th and 5th staff)\n",
        "-   41 is Violin (1st staff)\n",
        "-   42 is Viola (2nd staff)\n",
        "-   43 is Cello (3rd staff, upper voice)\n",
        "-   44 is Double Bass (3rd staff, lower voice)\n",
        "\n",
        "音価: 60がmiddle Cということを実際の譜面を見て確認。 この分析ではC4とし、半音あがることで1増える。\\\n",
        "1オクターブ12音あります。\n",
        "\n",
        "# データ変数と視覚変数の対応関係\n",
        "\n",
        "# 可視化作品と考察\n",
        "\n",
        "# コードかコードへのリンク（あれば）\n",
        "\n",
        "Github repository link: https://github.com/Higashiguchi-Rinhi/infovis-notebooks.git"
      ],
      "id": "f4250f1a"
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)",
      "path": "C:\\Users\\tanri\\AppData\\Local\\Programs\\Python\\Python312\\share\\jupyter\\kernels\\python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}