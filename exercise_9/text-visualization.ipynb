{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "523005b4",
   "metadata": {},
   "source": [
    "# テキストデータの可視化"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4748ec8f",
   "metadata": {},
   "source": [
    "[青空文庫](https://www.aozora.gr.jp/)に収録されている、夏目漱石の[『三四郎』](https://www.aozora.gr.jp/cards/000148/card794.html)という作品を使って、テキストデータの可視化を練習します。\n",
    "\n",
    "自然言語処理のフレームワークである[spaCy](https://spacy.io/)とこれを利用した日本語NLPライブラリである[GiNZA](https://megagonlabs.github.io/ginza/)を使います。\n",
    "\n",
    "![](https://spacy.io/pipeline-fde48da9b43661abcdf62ab70a546d71.svg)\n",
    "\n",
    "spaCyでは、各コンポーネント（機能）が順番に適用されるpipeline方式でテキストが処理されます。基本的には、tokenizer（分かち書き）、tagger（品詞付与）、parser（係り受け解析）、ner（固有表現抽出）、lemmatizer（原形抽出）、textcat（文書分類）というコンポーネントが用意されています。\n",
    "\n",
    "`nlp()`を実行するとデフォルトでtokenizer、tagger、parser、ner、lemmatizerが入力文書に適用されます。\n",
    "\n",
    "spaCyとGiNZAと比べて、より長く使われてきたライブラリとして、\n",
    "\n",
    "* [NLTK (Natural Language Toolkit)](https://www.nltk.org/index.html)\n",
    "* [MeCab](https://taku910.github.io/mecab/)（形態素解析） + [CaboCha](https://taku910.github.io/cabocha/)（係り受け解析）\n",
    "* [JUMAN](https://nlp.ist.i.kyoto-u.ac.jp/?JUMAN)（形態素解析） + [KNP](https://nlp.ist.i.kyoto-u.ac.jp/?KNP)（係り受け解析）\n",
    "\n",
    "などがあります。\n",
    "\n",
    "より高度な分析をする時に必要となることがあるので、ぜひチェックしてみてください。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "942b0f94",
   "metadata": {},
   "source": [
    "それでは、spaCyを使った形態素解析の方法を確認してから、テキストデータの可視化を行いましょう。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "079b9c80",
   "metadata": {},
   "source": [
    "## 形態素解析の練習\n",
    "\n",
    "形態素解析は、文章を一つ一つの形態素に分ける技術です。形態素は、「言葉が意味を持つまとまりの単語の最小単位」です。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "57f7f81d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting spacy\n",
      "  Downloading spacy-3.8.3-cp310-cp310-win_amd64.whl (12.2 MB)\n",
      "     ---------------------------------------- 12.2/12.2 MB 8.6 MB/s eta 0:00:00\n",
      "Collecting ginza\n",
      "  Using cached ginza-5.2.0-py3-none-any.whl (21 kB)\n",
      "Collecting ja-ginza\n",
      "  Using cached ja_ginza-5.2.0-py3-none-any.whl (59.1 MB)\n",
      "Collecting spacy-legacy<3.1.0,>=3.0.11\n",
      "  Using cached spacy_legacy-3.0.12-py2.py3-none-any.whl (29 kB)\n",
      "Collecting spacy-loggers<2.0.0,>=1.0.0\n",
      "  Using cached spacy_loggers-1.0.5-py3-none-any.whl (22 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\tanri\\infovis\\infovis-notebooks\\.venv\\lib\\site-packages (from spacy) (24.2)\n",
      "Collecting weasel<0.5.0,>=0.1.0\n",
      "  Using cached weasel-0.4.1-py3-none-any.whl (50 kB)\n",
      "Collecting typer<1.0.0,>=0.3.0\n",
      "  Using cached typer-0.15.1-py3-none-any.whl (44 kB)\n",
      "Collecting langcodes<4.0.0,>=3.2.0\n",
      "  Using cached langcodes-3.5.0-py3-none-any.whl (182 kB)\n",
      "Collecting thinc<8.4.0,>=8.3.0\n",
      "  Downloading thinc-8.3.3-cp310-cp310-win_amd64.whl (1.5 MB)\n",
      "     ---------------------------------------- 1.5/1.5 MB 10.5 MB/s eta 0:00:00\n",
      "Collecting requests<3.0.0,>=2.13.0\n",
      "  Using cached requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "Collecting jinja2\n",
      "  Using cached jinja2-3.1.4-py3-none-any.whl (133 kB)\n",
      "Collecting murmurhash<1.1.0,>=0.28.0\n",
      "  Downloading murmurhash-1.0.11-cp310-cp310-win_amd64.whl (25 kB)\n",
      "Collecting preshed<3.1.0,>=3.0.2\n",
      "  Downloading preshed-3.0.9-cp310-cp310-win_amd64.whl (122 kB)\n",
      "     -------------------------------------- 122.2/122.2 KB 7.5 MB/s eta 0:00:00\n",
      "Collecting pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4\n",
      "  Downloading pydantic-2.10.4-py3-none-any.whl (431 kB)\n",
      "     ------------------------------------- 431.8/431.8 KB 13.6 MB/s eta 0:00:00\n",
      "Collecting wasabi<1.2.0,>=0.9.1\n",
      "  Using cached wasabi-1.1.3-py3-none-any.whl (27 kB)\n",
      "Requirement already satisfied: setuptools in c:\\users\\tanri\\infovis\\infovis-notebooks\\.venv\\lib\\site-packages (from spacy) (58.1.0)\n",
      "Collecting catalogue<2.1.0,>=2.0.6\n",
      "  Using cached catalogue-2.0.10-py3-none-any.whl (17 kB)\n",
      "Collecting srsly<3.0.0,>=2.4.3\n",
      "  Downloading srsly-2.5.0-cp310-cp310-win_amd64.whl (632 kB)\n",
      "     ------------------------------------- 632.3/632.3 KB 13.2 MB/s eta 0:00:00\n",
      "Collecting cymem<2.1.0,>=2.0.2\n",
      "  Downloading cymem-2.0.10-cp310-cp310-win_amd64.whl (39 kB)\n",
      "Collecting tqdm<5.0.0,>=4.38.0\n",
      "  Downloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "     ---------------------------------------- 78.5/78.5 KB 4.3 MB/s eta 0:00:00\n",
      "Collecting numpy>=1.19.0\n",
      "  Downloading numpy-2.2.0-cp310-cp310-win_amd64.whl (12.9 MB)\n",
      "     --------------------------------------- 12.9/12.9 MB 13.9 MB/s eta 0:00:00\n",
      "Collecting SudachiPy<0.7.0,>=0.6.2\n",
      "  Downloading SudachiPy-0.6.9-cp310-cp310-win_amd64.whl (1.4 MB)\n",
      "     ---------------------------------------- 1.4/1.4 MB 12.3 MB/s eta 0:00:00\n",
      "Collecting SudachiDict-core>=20210802\n",
      "  Using cached SudachiDict_core-20241021-py3-none-any.whl (72.1 MB)\n",
      "Collecting plac>=1.3.3\n",
      "  Using cached plac-1.4.3-py2.py3-none-any.whl (22 kB)\n",
      "Collecting language-data>=1.2\n",
      "  Using cached language_data-1.3.0-py3-none-any.whl (5.4 MB)\n",
      "Collecting pydantic-core==2.27.2\n",
      "  Downloading pydantic_core-2.27.2-cp310-cp310-win_amd64.whl (2.0 MB)\n",
      "     ---------------------------------------- 2.0/2.0 MB 15.8 MB/s eta 0:00:00\n",
      "Collecting annotated-types>=0.6.0\n",
      "  Using cached annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in c:\\users\\tanri\\infovis\\infovis-notebooks\\.venv\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.12.2)\n",
      "Collecting idna<4,>=2.5\n",
      "  Using cached idna-3.10-py3-none-any.whl (70 kB)\n",
      "Collecting certifi>=2017.4.17\n",
      "  Downloading certifi-2024.12.14-py3-none-any.whl (164 kB)\n",
      "     -------------------------------------- 164.9/164.9 KB 9.7 MB/s eta 0:00:00\n",
      "Collecting charset-normalizer<4,>=2\n",
      "  Downloading charset_normalizer-3.4.0-cp310-cp310-win_amd64.whl (102 kB)\n",
      "     ---------------------------------------- 102.2/102.2 KB ? eta 0:00:00\n",
      "Collecting urllib3<3,>=1.21.1\n",
      "  Using cached urllib3-2.2.3-py3-none-any.whl (126 kB)\n",
      "Collecting confection<1.0.0,>=0.0.1\n",
      "  Using cached confection-0.1.5-py3-none-any.whl (35 kB)\n",
      "Collecting blis<1.2.0,>=1.1.0\n",
      "  Downloading blis-1.1.0-cp310-cp310-win_amd64.whl (6.4 MB)\n",
      "     ---------------------------------------- 6.4/6.4 MB 12.7 MB/s eta 0:00:00\n",
      "Requirement already satisfied: colorama in c:\\users\\tanri\\infovis\\infovis-notebooks\\.venv\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy) (0.4.6)\n",
      "Collecting click>=8.0.0\n",
      "  Using cached click-8.1.7-py3-none-any.whl (97 kB)\n",
      "Collecting shellingham>=1.3.0\n",
      "  Using cached shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n",
      "Collecting rich>=10.11.0\n",
      "  Using cached rich-13.9.4-py3-none-any.whl (242 kB)\n",
      "Collecting smart-open<8.0.0,>=5.2.1\n",
      "  Downloading smart_open-7.1.0-py3-none-any.whl (61 kB)\n",
      "     ---------------------------------------- 61.7/61.7 KB 3.2 MB/s eta 0:00:00\n",
      "Collecting cloudpathlib<1.0.0,>=0.7.0\n",
      "  Using cached cloudpathlib-0.20.0-py3-none-any.whl (52 kB)\n",
      "Collecting MarkupSafe>=2.0\n",
      "  Downloading MarkupSafe-3.0.2-cp310-cp310-win_amd64.whl (15 kB)\n",
      "Collecting marisa-trie>=1.1.0\n",
      "  Downloading marisa_trie-1.2.1-cp310-cp310-win_amd64.whl (151 kB)\n",
      "     -------------------------------------- 151.9/151.9 KB 9.4 MB/s eta 0:00:00\n",
      "Collecting markdown-it-py>=2.2.0\n",
      "  Using cached markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\tanri\\infovis\\infovis-notebooks\\.venv\\lib\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.18.0)\n",
      "Collecting wrapt\n",
      "  Downloading wrapt-1.17.0-cp310-cp310-win_amd64.whl (38 kB)\n",
      "Collecting mdurl~=0.1\n",
      "  Using cached mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Installing collected packages: SudachiPy, plac, cymem, wrapt, wasabi, urllib3, tqdm, SudachiDict-core, spacy-loggers, spacy-legacy, shellingham, pydantic-core, numpy, murmurhash, mdurl, MarkupSafe, marisa-trie, idna, cloudpathlib, click, charset-normalizer, certifi, catalogue, annotated-types, srsly, smart-open, requests, pydantic, preshed, markdown-it-py, language-data, jinja2, blis, rich, langcodes, confection, typer, thinc, weasel, spacy, ginza, ja-ginza\n",
      "Successfully installed MarkupSafe-3.0.2 SudachiDict-core-20241021 SudachiPy-0.6.9 annotated-types-0.7.0 blis-1.1.0 catalogue-2.0.10 certifi-2024.12.14 charset-normalizer-3.4.0 click-8.1.7 cloudpathlib-0.20.0 confection-0.1.5 cymem-2.0.10 ginza-5.2.0 idna-3.10 ja-ginza-5.2.0 jinja2-3.1.4 langcodes-3.5.0 language-data-1.3.0 marisa-trie-1.2.1 markdown-it-py-3.0.0 mdurl-0.1.2 murmurhash-1.0.11 numpy-2.2.0 plac-1.4.3 preshed-3.0.9 pydantic-2.10.4 pydantic-core-2.27.2 requests-2.32.3 rich-13.9.4 shellingham-1.5.4 smart-open-7.1.0 spacy-3.8.3 spacy-legacy-3.0.12 spacy-loggers-1.0.5 srsly-2.5.0 thinc-8.3.3 tqdm-4.67.1 typer-0.15.1 urllib3-2.2.3 wasabi-1.1.3 weasel-0.4.1 wrapt-1.17.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 22.0.4; however, version 24.3.1 is available.\n",
      "You should consider upgrading via the 'C:\\Users\\tanri\\infovis\\infovis-notebooks\\.venv\\Scripts\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikit-learn"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 22.0.4; however, version 24.3.1 is available.\n",
      "You should consider upgrading via the 'C:\\Users\\tanri\\infovis\\infovis-notebooks\\.venv\\Scripts\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Downloading scikit_learn-1.6.0-cp310-cp310-win_amd64.whl (11.1 MB)\n",
      "     ---------------------------------------- 11.1/11.1 MB 8.4 MB/s eta 0:00:00\n",
      "Collecting pandas\n",
      "  Downloading pandas-2.2.3-cp310-cp310-win_amd64.whl (11.6 MB)\n",
      "     ---------------------------------------- 11.6/11.6 MB 7.5 MB/s eta 0:00:00\n",
      "Collecting threadpoolctl>=3.1.0\n",
      "  Using cached threadpoolctl-3.5.0-py3-none-any.whl (18 kB)\n",
      "Requirement already satisfied: numpy>=1.19.5 in c:\\users\\tanri\\infovis\\infovis-notebooks\\.venv\\lib\\site-packages (from scikit-learn) (2.2.0)\n",
      "Collecting scipy>=1.6.0\n",
      "  Downloading scipy-1.14.1-cp310-cp310-win_amd64.whl (44.8 MB)\n",
      "     ---------------------------------------- 44.8/44.8 MB 7.4 MB/s eta 0:00:00\n",
      "Collecting joblib>=1.2.0\n",
      "  Using cached joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "Collecting pytz>=2020.1\n",
      "  Using cached pytz-2024.2-py2.py3-none-any.whl (508 kB)\n",
      "Collecting tzdata>=2022.7\n",
      "  Using cached tzdata-2024.2-py2.py3-none-any.whl (346 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\tanri\\infovis\\infovis-notebooks\\.venv\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\tanri\\infovis\\infovis-notebooks\\.venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Installing collected packages: pytz, tzdata, threadpoolctl, scipy, joblib, scikit-learn, pandas\n",
      "Successfully installed joblib-1.4.2 pandas-2.2.3 pytz-2024.2 scikit-learn-1.6.0 scipy-1.14.1 threadpoolctl-3.5.0 tzdata-2024.2\n"
     ]
    }
   ],
   "source": [
    "# ライブラリのインストール（初回のみ）\n",
    "\n",
    "!pip install spacy ginza ja-ginza\n",
    "!pip install scikit-learn pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e07ab651",
   "metadata": {},
   "source": [
    "### 日本語\n",
    "\n",
    "spaCyで日本語の形態素解析モデル（`ja_ginza`）をロードして、形態素解析をしてみましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b4567bd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "これ\n",
      "は\n",
      "文章\n",
      "です\n",
      "。\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"ja_ginza\")\n",
    "text = \"これは文章です。\"\n",
    "doc = nlp(text)\n",
    "for token in doc:\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af1ee04f",
   "metadata": {},
   "source": [
    "形態素解析の結果には、語の原形や品詞の情報も含まれます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "86d5d685",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "うとうと\tうとうと\tADV\t副詞\n",
      "と\tと\tADP\t助詞-格助詞\n",
      "し\tする\tAUX\t動詞-非自立可能\n",
      "て\tて\tSCONJ\t助詞-接続助詞\n",
      "目\t目\tNOUN\t名詞-普通名詞-一般\n",
      "が\tが\tADP\t助詞-格助詞\n",
      "さめる\tさめる\tVERB\t動詞-一般\n",
      "と\tと\tSCONJ\t助詞-格助詞\n",
      "女\t女\tNOUN\t名詞-普通名詞-一般\n",
      "は\tは\tADP\t助詞-係助詞\n",
      "いつ\tいつ\tPRON\t代名詞\n",
      "の\tの\tADP\t助詞-格助詞\n",
      "ま\tま\tNOUN\t名詞-普通名詞-助数詞可能\n",
      "に\tに\tADP\t助詞-格助詞\n",
      "か\tか\tADP\t助詞-副助詞\n",
      "、\t、\tPUNCT\t補助記号-読点\n",
      "隣\t隣\tNOUN\t名詞-普通名詞-一般\n",
      "の\tの\tADP\t助詞-格助詞\n",
      "じい\tじい\tNOUN\t名詞-普通名詞-一般\n",
      "さん\tさん\tNOUN\t接尾辞-名詞的-一般\n",
      "と\tと\tADP\t助詞-格助詞\n",
      "話\t話\tNOUN\t名詞-普通名詞-サ変可能\n",
      "を\tを\tADP\t助詞-格助詞\n",
      "始め\t始める\tVERB\t動詞-非自立可能\n",
      "て\tて\tSCONJ\t助詞-接続助詞\n",
      "いる\tいる\tVERB\t動詞-非自立可能\n",
      "。\t。\tPUNCT\t補助記号-句点\n",
      "この\tこの\tDET\t連体詞\n",
      "じい\tじい\tNOUN\t名詞-普通名詞-一般\n",
      "さん\tさん\tNOUN\t接尾辞-名詞的-一般\n",
      "は\tは\tADP\t助詞-係助詞\n",
      "たしか\tたしか\tADJ\t形状詞-一般\n",
      "に\tだ\tAUX\t助動詞\n",
      "前\t前\tNOUN\t名詞-普通名詞-副詞可能\n",
      "の\tの\tADP\t助詞-格助詞\n",
      "前\t前\tNOUN\t名詞-普通名詞-副詞可能\n",
      "の\tの\tADP\t助詞-格助詞\n",
      "駅\t駅\tNOUN\t名詞-普通名詞-一般\n",
      "から\tから\tADP\t助詞-格助詞\n",
      "乗っ\t乗る\tVERB\t動詞-一般\n",
      "た\tた\tAUX\t助動詞\n",
      "いなか者\tいなか者\tPROPN\t名詞-普通名詞-一般\n",
      "で\tだ\tAUX\t助動詞\n",
      "ある\tある\tVERB\t動詞-非自立可能\n",
      "。\t。\tPUNCT\t補助記号-句点\n",
      "発車\t発車\tNOUN\t名詞-普通名詞-サ変可能\n",
      "まぎわ\tまぎわ\tNOUN\t名詞-普通名詞-一般\n",
      "に\tに\tADP\t助詞-格助詞\n",
      "頓狂\t頓狂\tVERB\t形状詞-一般\n",
      "な\tだ\tAUX\t助動詞\n",
      "声\t声\tNOUN\t名詞-普通名詞-一般\n",
      "を\tを\tADP\t助詞-格助詞\n",
      "出し\t出す\tVERB\t動詞-非自立可能\n",
      "て\tて\tSCONJ\t助詞-接続助詞\n",
      "駆け込ん\t駆け込む\tVERB\t動詞-一般\n",
      "で\tで\tSCONJ\t助詞-接続助詞\n",
      "来\t来る\tVERB\t動詞-非自立可能\n",
      "て\tて\tSCONJ\t助詞-接続助詞\n",
      "、\t、\tPUNCT\t補助記号-読点\n",
      "いきなり\tいきなり\tADV\t副詞\n",
      "肌\t肌\tNOUN\t名詞-普通名詞-一般\n",
      "を\tを\tADP\t助詞-格助詞\n",
      "ぬい\tぬぐ\tVERB\t動詞-一般\n",
      "だ\tだ\tAUX\t助動詞\n",
      "と\tと\tADP\t助詞-格助詞\n",
      "思っ\t思う\tVERB\t動詞-一般\n",
      "たら\tた\tAUX\t助動詞\n",
      "背中\t背中\tNOUN\t名詞-普通名詞-一般\n",
      "に\tに\tADP\t助詞-格助詞\n",
      "お\tお\tNOUN\t接頭辞\n",
      "灸\t灸\tNOUN\t名詞-普通名詞-一般\n",
      "の\tの\tADP\t助詞-格助詞\n",
      "あと\tあと\tNOUN\t名詞-普通名詞-副詞可能\n",
      "が\tが\tADP\t助詞-格助詞\n",
      "いっぱい\tいっぱい\tADV\t副詞\n",
      "あっ\tある\tVERB\t動詞-非自立可能\n",
      "た\tた\tAUX\t助動詞\n",
      "の\tの\tSCONJ\t助詞-準体助詞\n",
      "で\tだ\tAUX\t助動詞\n",
      "、\t、\tPUNCT\t補助記号-読点\n",
      "三四郎\t三四郎\tPROPN\t名詞-固有名詞-人名-名\n",
      "の\tの\tADP\t助詞-格助詞\n",
      "記憶\t記憶\tNOUN\t名詞-普通名詞-サ変可能\n",
      "に\tに\tADP\t助詞-格助詞\n",
      "残っ\t残る\tVERB\t動詞-一般\n",
      "て\tて\tSCONJ\t助詞-接続助詞\n",
      "いる\tいる\tVERB\t動詞-非自立可能\n",
      "。\t。\tPUNCT\t補助記号-句点\n",
      "じい\tじい\tNOUN\t名詞-普通名詞-一般\n",
      "さん\tさん\tNOUN\t接尾辞-名詞的-一般\n",
      "が\tが\tADP\t助詞-格助詞\n",
      "汗\t汗\tNOUN\t名詞-普通名詞-一般\n",
      "を\tを\tADP\t助詞-格助詞\n",
      "ふい\tふく\tVERB\t動詞-一般\n",
      "て\tて\tSCONJ\t助詞-接続助詞\n",
      "、\t、\tPUNCT\t補助記号-読点\n",
      "肌\t肌\tNOUN\t名詞-普通名詞-一般\n",
      "を\tを\tADP\t助詞-格助詞\n",
      "入れ\t入れる\tVERB\t動詞-一般\n",
      "て\tて\tSCONJ\t助詞-接続助詞\n",
      "、\t、\tPUNCT\t補助記号-読点\n",
      "女\t女\tNOUN\t名詞-普通名詞-一般\n",
      "の\tの\tADP\t助詞-格助詞\n",
      "隣\t隣\tNOUN\t名詞-普通名詞-一般\n",
      "に\tに\tADP\t助詞-格助詞\n",
      "腰\t腰\tNOUN\t名詞-普通名詞-一般\n",
      "を\tを\tADP\t助詞-格助詞\n",
      "かけ\tかける\tVERB\t動詞-非自立可能\n",
      "た\tた\tAUX\t助動詞\n",
      "まで\tまで\tPART\t助詞-副助詞\n",
      "よく\tよく\tADV\t副詞\n",
      "注意\t注意\tVERB\t名詞-普通名詞-サ変可能\n",
      "し\tする\tAUX\t動詞-非自立可能\n",
      "て\tて\tSCONJ\t助詞-接続助詞\n",
      "見\t見る\tVERB\t動詞-非自立可能\n",
      "て\tて\tSCONJ\t助詞-接続助詞\n",
      "い\tいる\tVERB\t動詞-非自立可能\n",
      "た\tた\tAUX\t助動詞\n",
      "くらい\tくらい\tPART\t助詞-副助詞\n",
      "で\tだ\tAUX\t助動詞\n",
      "ある\tある\tVERB\t動詞-非自立可能\n",
      "。\t。\tPUNCT\t補助記号-句点\n"
     ]
    }
   ],
   "source": [
    "text = \"うとうととして目がさめると女はいつのまにか、隣のじいさんと話を始めている。このじいさんはたしかに前の前の駅から乗ったいなか者である。発車まぎわに頓狂な声を出して駆け込んで来て、いきなり肌をぬいだと思ったら背中にお灸のあとがいっぱいあったので、三四郎の記憶に残っている。じいさんが汗をふいて、肌を入れて、女の隣に腰をかけたまでよく注意して見ていたくらいである。\"\n",
    "doc = nlp(text)\n",
    "for token in doc:\n",
    "    print(f\"{token}\\t{token.lemma_}\\t{token.pos_}\\t{token.tag_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68e5afef",
   "metadata": {},
   "source": [
    "### 英語\n",
    "\n",
    "spaCyでは、モデルを変えるだけで同じ手順で他言語の解析を行うことができます。\n",
    "\n",
    "英語のモデルでも試してみましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4b3b858d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
      "     ---------------------------------------- 12.8/12.8 MB 9.4 MB/s eta 0:00:00\n",
      "Installing collected packages: en-core-web-sm\n",
      "Successfully installed en-core-web-sm-3.8.0\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 22.0.4; however, version 24.3.1 is available.\n",
      "You should consider upgrading via the 'c:\\Users\\tanri\\infovis\\infovis-notebooks\\.venv\\Scripts\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "# 英語のモデルをダウンロードする\n",
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e16440c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This\tthis\tPRON\tDT\n",
      "is\tbe\tAUX\tVBZ\n",
      "a\ta\tDET\tDT\n",
      "sentence\tsentence\tNOUN\tNN\n",
      ".\t.\tPUNCT\t.\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "text = \"This is a sentence.\"\n",
    "doc = nlp(text)\n",
    "for token in doc:\n",
    "    print(f\"{token}\\t{token.lemma_}\\t{token.pos_}\\t{token.tag_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cec4c56e",
   "metadata": {},
   "source": [
    "## テキストデータの可視化"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b865a216",
   "metadata": {},
   "source": [
    "それでは、『三四郎』を題材にテキストデータの可視化を行ってみましょう。\n",
    "\n",
    "* ワードクラウド\n",
    "* 共起ネットワーク\n",
    "* 共起マトリックス\n",
    "\n",
    "の3種類を紹介します。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afe05fba",
   "metadata": {},
   "source": [
    "### データの用意"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc88a8b0",
   "metadata": {},
   "source": [
    "まずは、青空文庫からテキストデータをダウンロードします。解凍されたファイルの文字コードがShift-JISになっている点に注意しましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0c75a244",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "100  154k  100  154k    0     0   448k      0 --:--:-- --:--:-- --:--:--  455k\n",
      "A subdirectory or file text already exists.\n"
     ]
    }
   ],
   "source": [
    "# ファイルをダウンロードする\n",
    "!curl https://www.aozora.gr.jp/cards/000148/files/794_ruby_4237.zip -o 794_ruby_4237.zip\n",
    "# textフォルダ作る\n",
    "!mkdir text\n",
    "# ファイルをtextフォルダに解凍\n",
    "# !unzip -d text -o 794_ruby_4237.zip\n",
    "!tar -xf 794_ruby_4237.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeca434f",
   "metadata": {},
   "source": [
    "次に、正規表現で青空文庫のルビ、注、アクセントの記号を取り除きます。\n",
    "\n",
    "同時に、文字コードをShift-JISからUTF-8にします。\n",
    "\n",
    "```\n",
    "\n",
    "-------------------------------------------------------\n",
    "【テキスト中に現れる記号について】\n",
    "\n",
    "《》：ルビ\n",
    "（例）頓狂《とんきょう》\n",
    "\n",
    "｜：ルビの付く文字列の始まりを特定する記号\n",
    "（例）福岡県｜京都郡《みやこぐん》\n",
    "\n",
    "［＃］：入力者注　主に外字の説明や、傍点の位置の指定\n",
    "　　　（数字は、JIS X 0213の面区点番号またはUnicode、底本のページと行数）\n",
    "（例）※［＃「魚＋師のつくり」、第4水準2-93-37］\n",
    "\n",
    "〔〕：アクセント分解された欧文をかこむ\n",
    "（例）〔ve'rite'《ヴェリテ》 vraie《ヴレイ》.〕\n",
    "アクセント分解についての詳細は下記URLを参照してください\n",
    "http://www.aozora.gr.jp/accent_separation.html\n",
    "-------------------------------------------------------\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9be8d4ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "input_fn = \"text/sanshiro.txt\"\n",
    "output_fn = \"text/sanshiro.stripruby.txt\"\n",
    "\n",
    "with open(input_fn, encoding=\"shift_jis\") as fin, open(output_fn, mode=\"w\") as fout:\n",
    "    for line in fin:\n",
    "        fout.write(re.sub(r\"《[^》]+》|［[^］]+］|〔[^〕]+〕| [｜]\", \"\", line))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94129c7b",
   "metadata": {},
   "source": [
    "冒頭と末尾の説明を取り除きます（何行取り除くかは目視で確認）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bda308ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'tail' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "# macOSの場合は、homebrewでcoreutilsをインストールする必要があります（gheadを使うため）\n",
    "# homebrewがインストールされていない場合は、https://brew.sh/からインストールしてください\n",
    "\n",
    "if sys.platform == \"darwin\":\n",
    "    !brew install coreutils\n",
    "    !tail -n +22 text/sanshiro.stripruby.txt | ghead -n -14 > text/sanshiro.corpus.txt\n",
    "else:\n",
    "    !tail -n +22 text/sanshiro.stripruby.txt | head -n -14 > text/sanshiro.corpus.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9df3b89d",
   "metadata": {},
   "source": [
    "これで、テキストファイルを扱う準備ができました。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5840589",
   "metadata": {},
   "source": [
    "### ワードクラウド\n",
    "\n",
    "ワードクラウドは、形態素解析で得られた頻出単語の頻出度合いを文字の大きさで可視化する手法です。頻度の高い単語を大きく表示することで、テキスト全体の傾向を素早く理解することができます。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1a55698",
   "metadata": {},
   "source": [
    "#### 形態素解析"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abeb0eee",
   "metadata": {},
   "source": [
    "それでは、『三四郎』に出現する単語の頻度を数えてみましょう。\n",
    "\n",
    "テキストファイルを読み込んで形態素解析を行います。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "16bc7fb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "input_fn = \"text/sanshiro.corpus.txt\"\n",
    "output_fn = \"text/sanshiro.wakati.txt\"\n",
    "\n",
    "nlp = spacy.load(\"ja_ginza\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ed6f214c",
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = []\n",
    "with open(input_fn, \"r\", encoding=\"shift-jis\") as fin:\n",
    "    for line in fin:\n",
    "        lines.append(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "641306b0",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "with open(output_fn, \"a\", encoding=\"shift-jis\") as fout:\n",
    "    for line in lines:\n",
    "        tokens = [token.text for token in nlp(line.rstrip())]\n",
    "    \n",
    "        fout.write(' '.join(tokens) + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fd5a14f",
   "metadata": {},
   "source": [
    "出力されたファイルを確認すると、単語分割されていることが分かります。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "396e04dc",
   "metadata": {},
   "source": [
    "次に、使用頻度の高い単語を見てみましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "35165aec",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# ファイルを読み込み、テキストを一行ずつ解析\n",
    "all_tokens = []\n",
    "with open(input_fn, \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        tokens = [token for token in nlp(line)]\n",
    "        all_tokens.extend(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e1bcc22d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'all_tokens' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m stopwords \u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mする\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mある\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mない\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mいう\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mもの\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mこと\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mよう\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mなる\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mほう\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mいる\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mくる\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mさん\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# 単語の頻度を数える\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m counter \u001b[38;5;241m=\u001b[39m Counter(token\u001b[38;5;241m.\u001b[39mlemma_ \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m \u001b[43mall_tokens\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m token\u001b[38;5;241m.\u001b[39mpos_ \u001b[38;5;129;01min\u001b[39;00m include_pos \u001b[38;5;129;01mand\u001b[39;00m token\u001b[38;5;241m.\u001b[39mlemma_ \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m stopwords)\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# 出現頻度top 20を出力する\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m word, count \u001b[38;5;129;01min\u001b[39;00m counter\u001b[38;5;241m.\u001b[39mmost_common(\u001b[38;5;241m20\u001b[39m):\n",
      "\u001b[1;31mNameError\u001b[0m: name 'all_tokens' is not defined"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# 分析対象とする品詞（内容語 - 名詞、動詞、形容詞）と不要語（ストップワード）を指定する\n",
    "include_pos = (\"NOUN\", \"VERB\", \"ADJ\")\n",
    "stopwords = (\"する\", \"ある\", \"ない\", \"いう\", \"もの\", \"こと\", \"よう\", \"なる\", \"ほう\", \"いる\", \"くる\", \"さん\")\n",
    "\n",
    "# 単語の頻度を数える\n",
    "counter = Counter(token.lemma_ for token in all_tokens if token.pos_ in include_pos and token.lemma_ not in stopwords)\n",
    "\n",
    "# 出現頻度top 20を出力する\n",
    "for word, count in counter.most_common(20):\n",
    "    print(f\"{count:>5} {word}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b152e15c",
   "metadata": {},
   "source": [
    "#### ワードクラウドの作成"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52b9e8fb",
   "metadata": {},
   "source": [
    "それでは、このデータをもとにワードクラウドを作成します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04f22e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ライブラリをインストールする\n",
    "!pip install wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11ca7261",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 日本語フォントのダウンロード（Linuxのみ）\n",
    "\n",
    "if sys.platform == \"linux\":\n",
    "    !sudo apt update\n",
    "    !sudo apt install fonts-ipaexfont"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6230c37c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# フォントファイルの場所の指定（図が上手く表示されない場合は、書き換えてください）\n",
    "\n",
    "if sys.platform == \"darwin\":\n",
    "    fpath = \"/Library/Fonts/Arial Unicode.ttf\"\n",
    "else:\n",
    "    fpath = \"/usr/share/fonts/opentype/ipaexfont-gothic/ipaexg.ttf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38faa10b",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = [token.lemma_ for token in all_tokens if token.pos_ in include_pos and token.lemma_ not in stopwords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abaf93da",
   "metadata": {},
   "outputs": [],
   "source": [
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d98a1286",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "# ワードクラウドの表示設定と作成\n",
    "wordcloud = WordCloud(\n",
    "    width=1600, height=800,\n",
    "    background_color=\"white\", font_path=fpath\n",
    ").generate(' '.join(words))\n",
    "\n",
    "# プロット\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.tight_layout(pad=0)\n",
    "plt.savefig(\"wordcloud.png\", dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "599b22f7",
   "metadata": {},
   "source": [
    "「先生」、「言う」、「女」、「見る」などが高頻度で出現していることが分かります。\n",
    "\n",
    "分析対象の品詞に、固有名詞を加えてみたらどうでしょうか？試してみましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57d298b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 分析対象とする品詞と不要語を指定する\n",
    "# your code goes in ????? below\n",
    "\n",
    "include_pos = (\"NOUN\", \"VERB\", \"ADJ\", \"?????\")\n",
    "stopwords = (\"する\", \"ある\", \"ない\", \"いう\", \"もの\", \"こと\", \"よう\", \"なる\", \"ほう\", \"いる\", \"くる\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19ca2a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = [token.lemma_ for token in all_tokens if token.pos_ in include_pos and token.lemma_ not in stopwords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2ad7e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ワードクラウドの表示設定と作成\n",
    "wordcloud = WordCloud(\n",
    "    width=1600, height=800,\n",
    "    background_color=\"white\", font_path=fpath\n",
    ").generate(' '.join(words))\n",
    "\n",
    "# プロット\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.tight_layout(pad=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58d2909c",
   "metadata": {},
   "source": [
    "### 共起ネットワーク\n",
    "\n",
    "次に、共起ネットワークを作って、どの語とどの語が一緒に使われているかを調べてみましょう。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88b27f9a",
   "metadata": {},
   "source": [
    "#### 共起分析\n",
    "\n",
    "まず、共起分析を行います。文章を文に分割し、同一文中に同時に出現する単語の組を数え上げることで分析します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bc555471",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1248d0e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_words(sent, pos_tags, stopwords):\n",
    "    \"\"\"\n",
    "    分析対象の品詞であり、不要語ではない単語を抽出する\n",
    "    \"\"\"\n",
    "    words = [token.lemma_ for token in sent if token.pos_ in pos_tags and token.lemma_ not in stopwords]\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f96dfa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_cooccurrence(sents, token_length=\"{2,}\"):\n",
    "    \"\"\"\n",
    "    同じ文中に共起する単語を行列形式で列挙する\n",
    "    \"\"\"\n",
    "    token_pattern = f\"\\\\b\\\\w{token_length}\\\\b\"\n",
    "    count_model = CountVectorizer(token_pattern=token_pattern)\n",
    "\n",
    "    X = count_model.fit_transform(sents)\n",
    "    words = count_model.get_feature_names_out()\n",
    "    word_counts = np.asarray(X.sum(axis=0)).reshape(-1)\n",
    "\n",
    "    X[X > 0] = 1 # 同じ共起が2以上出現しても1とする\n",
    "    Xc = (X.T * X) # 共起行列を求めるための掛け算をする、csr形式の疎行列\n",
    "\n",
    "    return words, word_counts, Xc, X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "94cfc975",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_sentence_by_cooccurrence(X, idxs):\n",
    "    \"\"\"\n",
    "    指定された共起を含む文を見つける\n",
    "    \"\"\"\n",
    "    occur_flags = (X[:, idxs[0]] > 0)\n",
    "    for idx in idxs[1:]:\n",
    "        occur_flags = occur_flags.multiply(X[:, idx] > 0)\n",
    "\n",
    "    return occur_flags.nonzero()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "557a60ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 文章を解析し、共起を求める\n",
    "include_pos = (\"NOUN\", \"VERB\", \"ADJ\", \"PROPN\")\n",
    "stopwords = (\"する\", \"ある\", \"ない\", \"いう\", \"もの\", \"こと\", \"よう\", \"なる\", \"ほう\", \"いる\", \"くる\", \"さん\")\n",
    "\n",
    "sents = []\n",
    "with open(input_fn, \"r\") as f:\n",
    "    for line in f:\n",
    "        doc = nlp(line)\n",
    "        tmp = [' '.join(extract_words(sent, include_pos, stopwords)) for sent in doc.sents]\n",
    "        sents.extend(tmp)\n",
    "\n",
    "words, word_counts, Xc, X = count_cooccurrence(sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e4b8ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 共起ランキングを出力する\n",
    "# 共起行列Xcは疎行列なので、非ゼロ要素のみをカウンタに格納する\n",
    "counter = Counter()\n",
    "for i, j in zip(*Xc.nonzero()):\n",
    "    if i >= j:\n",
    "        continue\n",
    "    counter[(i, j)] += Xc[i, j]\n",
    "\n",
    "# 共起の出現頻度top 20を出力する\n",
    "for (i, j), c in counter.most_common(20):\n",
    "    print(f\"{c:>3d} ({words[i]}, {words[j]})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b7bec19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 指定した共起を含む文のリストを出力する\n",
    "sents_orig = []\n",
    "with open(input_fn, \"r\") as f:\n",
    "    for line in f:\n",
    "        doc = nlp(line)\n",
    "        tmp = list(doc.sents)\n",
    "        sents_orig.extend(tmp)\n",
    "\n",
    "# すべての単語の通し番号を求める\n",
    "words_lookup = { word: index for index, word in enumerate(words) }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fde1d4c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 共起語を指定する\n",
    "lookup_words = [\"野々宮\", \"美禰子\"]\n",
    "\n",
    "# 指定した共起語のインデックスを求める\n",
    "idxs = list(map(lambda x: words_lookup[x], lookup_words))\n",
    "\n",
    "# 指定した共起を含む文のリストを出力する\n",
    "for i in find_sentence_by_cooccurrence(X, idxs):\n",
    "    print(f\"{i:>5d}: {sents_orig[i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e8bef69",
   "metadata": {},
   "source": [
    "#### 共起ネットワークの作成"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01164062",
   "metadata": {},
   "source": [
    "共起分析の結果に基づいて共起ネットワークを作成します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "790e9ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install networkx pyvis japanize_matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5af921ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "from pyvis.network import Network\n",
    "import japanize_matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35721025",
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_weights(words, word_counts):\n",
    "    \"\"\"\n",
    "    単語の最多頻度が1となるような相対値として単語の重みを求める\n",
    "    \"\"\"\n",
    "    count_max = word_counts.max()\n",
    "    weights = [(word, {\"weight\": count / count_max})\n",
    "               for word, count in zip(words, word_counts)]\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "722233eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cooccurrence_weights(words, Xc, weight_cutoff):\n",
    "    \"\"\"\n",
    "    共起の最多頻度が1となるような相対値として共起の重みを求める\n",
    "    共起の重みがweight_cutoffより低い共起は除外する\n",
    "    \"\"\"\n",
    "    Xc_max = Xc.max()\n",
    "    cutoff = weight_cutoff * Xc_max\n",
    "    weights = [(words[i], words[j], Xc[i, j] / Xc_max)\n",
    "               for i, j in zip(*Xc.nonzero()) if i < j and Xc[i, j] > cutoff]\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4b0afbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_network(words, word_counts, Xc, weight_cutoff):\n",
    "    \"\"\"\n",
    "    語、単語頻度、共起行列から共起ネットワークをNetworkX形式で得る\n",
    "    \"\"\"\n",
    "    G = nx.Graph()\n",
    "\n",
    "    weights_w = word_weights(words, word_counts)\n",
    "    G.add_nodes_from(weights_w)\n",
    "\n",
    "    weights_c = cooccurrence_weights(words, Xc, weight_cutoff)\n",
    "    G.add_weighted_edges_from(weights_c)\n",
    "\n",
    "    G.remove_nodes_from(list(nx.isolates(G)))\n",
    "    return G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ca6c99f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pyplot_network(G):\n",
    "    \"\"\"\n",
    "    NetworkX形式で与えられた共起ネットワークをpyplotで描画する\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    pos = nx.spring_layout(G, k=0.1)\n",
    "\n",
    "    weights_n = np.array(list(nx.get_node_attributes(G, \"weight\").values()))\n",
    "    nx.draw_networkx_nodes(G, pos, node_size=300 * weights_n)\n",
    "    weights_e = np.array(list(nx.get_edge_attributes(G, \"weight\").values()))\n",
    "    nx.draw_networkx_edges(G, pos, width=20 * weights_e)\n",
    "\n",
    "    nx.draw_networkx_labels(G, pos, font_family=\"IPAexGothic\")\n",
    "\n",
    "    plt.axis(\"off\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5066c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nx2pyvis_G(G):\n",
    "    \"\"\"\n",
    "    NetworkX形式で与えられた共起ネットワークをpyvisで描画する\n",
    "    \"\"\"\n",
    "    pyvis_G = Network(width=\"800px\", height=\"800px\", notebook=True)\n",
    "    for node, attrs in G.nodes(data=True):\n",
    "        pyvis_G.add_node(node, title=node, size=30 * attrs[\"weight\"])\n",
    "    for node1, node2, attrs in G.edges(data=True):\n",
    "        pyvis_G.add_edge(node1, node2, width=20 * attrs[\"weight\"])\n",
    "\n",
    "    return pyvis_G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22c06913",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ネットワークを作る\n",
    "G = create_network(words, word_counts, Xc, 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fca19a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 静的ビジュアライゼーション\n",
    "pyplot_network(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33ef3fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# インタラクティブなビジュアライゼーション\n",
    "pyvis_G = nx2pyvis_G(G)\n",
    "pyvis_G.show_buttons()\n",
    "pyvis_G.show(\"network.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baf4ca48",
   "metadata": {},
   "source": [
    "このファイルを同じディレクトリに保存された`network.html`をブラウザで開き、インタラクティブな共起ネットワークを確認してみてください。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28da92e0",
   "metadata": {},
   "source": [
    "### 共起ヒートマップ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaf64940",
   "metadata": {},
   "source": [
    "共起関係をネットワークではなく、ヒートマップとして表現することもできます。\n",
    "\n",
    "登場人物の共起ヒートマップを作ってみましょう。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ce90a36",
   "metadata": {},
   "source": [
    "#### データフレームを作る"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faf4901e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 文章を解析し、共起を求める\n",
    "include_pos = (\"NOUN\", \"PROPN\")\n",
    "stopwords = (\"する\", \"ある\", \"ない\", \"いう\", \"もの\", \"こと\", \"よう\", \"なる\", \"ほう\", \"いる\", \"くる\", \"さん\")\n",
    "\n",
    "sents = []\n",
    "with open(input_fn, \"r\") as f:\n",
    "    for line in f:\n",
    "        doc = nlp(line)\n",
    "        tmp = [' '.join(extract_words(sent, include_pos, stopwords)) for sent in doc.sents]\n",
    "        sents.extend(tmp)\n",
    "\n",
    "words, word_counts, Xc, X = count_cooccurrence(sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f46039f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 共起行列Xcは疎行列なので、非ゼロ要素のみをカウンタに格納する\n",
    "counter = Counter()\n",
    "for i, j in zip(*Xc.nonzero()):\n",
    "    if i >= j:\n",
    "        continue\n",
    "    counter[(i, j)] += Xc[i, j]\n",
    "\n",
    "# 共起の出現頻度top 20を出力する\n",
    "for (i, j), c in counter.most_common(20):\n",
    "    print(f\"{c:>3d} ({words[i]}, {words[j]})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a51d4dc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 行名を用意する\n",
    "\n",
    "columns = set(Xc.nonzero()[0])\n",
    "columns_text = [words[i] for i in columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42569a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# すべて0のデータフレームを用意する\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(np.zeros((len(columns), len(columns))), index=columns_text, columns=columns_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f488da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# データフレームに頻度を入れる\n",
    "\n",
    "for cord, count in counter.items():\n",
    "    df.iloc[cord] = count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "865a9c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 登場人物名のみを取り出す\n",
    "\n",
    "characters = [\"三四郎\", \"広田\", \"野々宮\", \"佐々木\", \"与次郎\", \"美禰子\", \"先生\", \"原口\", \"里見\"]\n",
    "df_characters = df[characters].filter(items=characters, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c693323",
   "metadata": {},
   "source": [
    "#### 共起ヒートマップの作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "695614a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install plotly kaleido"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e752f2e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "fig = px.imshow(df_characters, color_continuous_scale=px.colors.sequential.Oranges,\n",
    "                title=\"『三四郎』の登場人物\", width=800, height=800)\n",
    "fig.update_layout(font=dict(size=16))\n",
    "fig.show()\n",
    "fig.write_image(\"heatmap.png\", scale=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85cbca9a",
   "metadata": {},
   "source": [
    "人々のインタラクションの多寡が示されました。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27ca8e60",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0303ca15",
   "metadata": {},
   "source": [
    "これらの可視化手法は文学作品だけではなく、他の分野のテキストデータにも有効です。\n",
    "\n",
    "文学作品に限っていうと、複数の作家の作品群を比較すると面白い結果が出るかもしれません。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
